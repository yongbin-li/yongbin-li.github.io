<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title> Yongbin Li Homepage </title>
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link href="./css" rel="stylesheet">
    <style type="text/css">
        * {
            margin: 0;
            padding: 0;
        }

        body {
            font: 16px Helvetica, Sans-Serif;
            line-height: 24px;
        }

        /*background: url(images/background.jpg); }*/
        .clear {
            clear: both;
        }

        #page-wrap {
            width: 900px;
            margin: 40px auto 60px;
        }

        #pic {
            float: right;
            margin: -30px 0 0 0;
        }

        h1 {
            margin: 0 0 16px 0;
            padding: 0 0 16px 0;
            font-size: 42px;
            letter-spacing: -2px;
            /* border-bottom: 1px solid #999; */
            font-weight: normal;
            font-family: "hind"
        }

        h2 {
            font-size: 20px;
            margin: 0 0 6px 0;
            position: relative;
        }

        h2 span {
            position: absolute;
            bottom: 0;
            right: 0;
            font-style: italic;
            font-family: Georgia, Serif;
            font-size: 16px;
            color: #999;
            font-weight: normal;
        }

        p {
            margin: 0 0 1px 0;
        }

        a {
            text-decoration: none;
            border-bottom: 1px;
        }

        a:hover {
            border-bottom-style: solid;
            color: black;
        }

        ul {
            margin: 0 0 32px 17px;
        }

        #objective {
            width: 600px;
            float: left;
        }

        #objective p {
            font-family: 16px Helvetica, sans-serif;
        }

        dt {
            font-style: italic;
            font-weight: bold;
            font-size: 18px;
            text-align: right;
            padding: 0 26px 0 0;
            width: 150px;
            float: left;
            height: 100px;
            border-right: 1px solid #999;
        }

        dd {
            width: 600px;
            float: right;
        }

        dd.clear {
            float: none;
            margin: 0;
            height: 15px;
        }

        img.borderedpicture {
            background-color: #7d7d7d;
            background-image: url(texture tile graphic‚Äôs address);
            display: block;
            width: 100%;
            max-width: 100%;
            height: auto;
            size: cover;
            padding: 0 0 0 0;
            margin: 0 0 0 0;
            box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
        }

        a:hover,a:visited{color: 	blue;}
    </style>
    <link rel="preconnect" href="https://fonts.gstatic.com/">
</head>



<body>

    <div id="page-wrap">

        <div style="float:right;margin-top: 100px"><img class="img-fluid z-depth-1 rounded borderedpicture"
                src="./shuide.jpg" alt="Photo of Yongbin Li" id="pic" style="width:240px;height:324px;"></div>

        <div id="contact-info" class="vcard">

            <!-- Microformats! -->

            <h1><b>Yongbin Li / ÊùéÊ∞∏ÂΩ¨ </b></h1>
            <p> <b><code class="language-plaintext highlighter-rouge">Alibaba DAMO Academy</code></b></p>
            <p><small> <code class="language-plaintext highlighter-rouge">liyb821 [at] qq.com</code></small></p>
            <a href="https://scholar.google.com/citations?user=xF5VrokAAAAJ&hl=en"> <small>Google Scholar </small></a> /
            <a href="https://scholar.google.com/citations?hl=en&user=5QkHNpkAAAAJ"> <small>Team's Google Scholar </small></a> /
            <a href="https://github.com/AlibabaResearch/DAMO-ConvAI"> <small>Team's Github </small></a>
        </div>

        <div id="objective">
            <p style="margin-top: 10px">
                Yongbin Li, graduated from Tsinghua University, is working in Alibaba DAMO Academy. My research direction is <b> LLMs, Conversational AI, Code and Agents </b>. I am responsible for the application of the aforementioned technologies for products such as: <a href="https://tongyi.aliyun.com/lingma">Tongyi Lingma</a>, <a href="https://tongyi.aliyun.com/xiaomi">Tongyi Xiaomi</a>, <a href="https://tongyi.aliyun.com/lingma">Tongyi Tingwu</a>, <a href="https://bailian.aliyun.com">Aliyun Bailian</a>, <a href="https://page.dingtalk.com/wow/dingtalk/default/dingtalk/I0HfYX4QStBIpLgxnZQe">Dingtalk Magic</a>. In recent years, I have published <font color=red><b>60+</b></font> <b>top conference papers as corresponding author</b>, including ACL/EMNLP/NeurIPS/AAAI/SIGIR/KDD, etc. <br>
                <br>
                ÊùéÊ∞∏ÂΩ¨ÔºåÊØï‰∏ö‰∫éÊ∏ÖÂçéÂ§ßÂ≠¶ÔºåÈòøÈáåÂ∑¥Â∑¥ËææÊë©Èô¢„ÄÇ‰∏ªË¶ÅÁ†îÁ©∂ÊñπÂêëÂåÖÊã¨<b>Â§ßÊ®°Âûã„ÄÅÂØπËØùÊô∫ËÉΩ„ÄÅ‰ª£Á†ÅÊô∫ËÉΩÂíåAI Agent</b>ÔºåÂπ∂Âõ¥Áªï‰∏äËø∞ÊäÄÊúØÊâìÈÄ†‰∫Ü <a href="https://tongyi.aliyun.com/lingma">ÈÄö‰πâÁÅµÁ†Å</a>Ôºå<a href="https://tongyi.aliyun.com/xiaomi">ÈÄö‰πâÊôìËúú</a>Ôºå<a href="https://tongyi.aliyun.com/lingma">ÈÄö‰πâÂê¨ÊÇü</a>Ôºå<a href="https://bailian.aliyun.com">ÈòøÈáå‰∫ëÁôæÁÇº</a>Ôºå <a href="https://page.dingtalk.com/wow/dingtalk/default/dingtalk/I0HfYX4QStBIpLgxnZQe">ÈíâÈíâÈ≠îÊ≥ïÊ£í</a>Á≠âÂ§ßÊ®°Âûã‰∫ßÂìÅÂ∫îÁî®. ËøëÂπ¥Êù•Ôºå<b>‰ª•ÈÄöËÆØ‰ΩúËÄÖË∫´‰ªΩÂèëË°® <font color=red>60+</font> ÁØáÈ°∂‰ºöËÆ∫Êñá</b>ÔºåÂåÖÊã¨ ACL/EMNLP/NeurIPS/AAAI/SIGIR/KDDÁ≠â‰ºöËÆÆ„ÄÇ <br>
            </p>
        </div>

        <div style="margin-top: 300px">
            <h2 id="news" style="margin-top: 30px"> üî• News</h2>
            <ul class="news">
                <li><b>„Äê2024-02„Äë</b>: <font color=red><b> 5 papers are accepted by COLING 2024 ÔºÅ</font></b> <br></li>
                <li><b>„Äê2023-12„Äë</b>: <font color=red><b> meta-reviewer for NAACL 2024 </font></b> <br></li>
                <li><b>„Äê2023-10„Äë</b>: <font color=red><b> 7 papers are accepted by EMNLP 2023 ÔºÅ</font></b> <br></li>
                <li><b>„Äê2023-09„Äë</b>: BIRD-SQL is accepted by <font color=red><b>  NeurIPS 2023 Spotlight ÔºÅ</font></b> <br></li>
                <li><b>„Äê2023-08„Äë</b>: SigDial 2023 DSTC11 workshop <font color=red><b>  BEST PAPER ÔºÅ</font></b> <br></li>
                <li><b>„Äê2023-05„Äë</b>: <font color=red><b> 10 papers are accepted by ACL 2023 ÔºÅ</font></b> <br></li>
                <li><b>„Äê2022-10„Äë</b>: <font color=red><b> 10 papers are accepted by EMNLP 2022 ÔºÅ</font></b> <br></li>
                <li><b>„Äê2022-08„Äë</b>: SPACE 2.0 was accepted by COLING 2022 and was recommended for Best Paper Award ÔºÅ <br></li>
                <li><b>„Äê2022-08„Äë</b>: SUNSQL was accepted by COLING 2022 and was recommended for Best Paper Award ÔºÅ <br></li>
                <li><b>„Äê2022-06„Äë</b>: meta-reviewer for WSDM 2023</li>
            </ul>

            <h2 id="publication" style="margin-top: 50px"> üìù Selected Publications </h2>
            <p style="color:#7d7d7d"> * indicates equal contribution </p>
            <ul class="pub">

                 <li>
                    <b><b style="color:purple;">SPACE-1</b> : A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. </b>
                    <br>
                    Wanwei He*, Yinpei Dai*, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2111.14592.pdf">Paper</a>]
                    [<a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/space-1">Code</a>]
                    <b style="color: 	#FF4500;"> [AAAI 2022] </b>
                 </li><br>

                 <li>
                    <b><b style="color:purple;">SPACE-2</b> : Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding. </b>
                    <br>
                    Wanwei He*, Yinpei Dai*, Binyuan Hui*, Min Yang, Zheng Cao, Jianbo Dong, Jian SUN, Fei Huang, Luo Si and  Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2209.06638.pdf">Paper</a>]
                    [<a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/space-2">Code</a>]
                    <b style="color: 	#FF4500;"> [COLING 2022] </b>
                 </li><br>

                 <li>
                    <b><b style="color:purple;">SPACE-3</b> : Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation. </b>
                    <br>
                    Wanwei He*, Yinpei Dai*, Min Yang, Jian Sun, Fei Huang, Luo Si and Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/abs/2209.06664.pdf">Paper</a>]
                    [<a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/space-3">Code</a>]
                    <b style="color: 	#FF4500;"> [SIGIR 2022] </b>
                </li><br>

                <li>
                    <b> PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts </b>
                    <br>
                    Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2305.14839.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [ACL 2023] </b>
                </li><br>

                <li>
                    <b> Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. </b>
                    <br>
                    Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://aclanthology.org/2023.acl-long.438.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [ACL 2022] </b>
                </li><br>

                <li>
                    <b> Api-bank: A benchmark for tool-augmented llms. </b>
                    <br>
                    Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2304.08244.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2023] </b>
                </li><br>

                <li>
                    <b> Preference ranking optimization for human alignment. </b>
                    <br>
                    Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang
                    <br>

                    [<a href="https://arxiv.org/pdf/2306.17492.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [AAAI 2024] </b>
                </li><br>

                <li>
                    <b> Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. </b>
                    <br>
                    Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Chenhao Ma, Kevin CC Chang, Fei Huang, Reynold Cheng, Yongbin Li
                    <br>

                    [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/83fc8fab1710363050bbd1d4b8cc0021-Paper-Datasets_and_Benchmarks.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [NeurIPS 2023] </b>
                </li><br>

                <li>
                    <b> SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains. </b>
                    <br>
                    Shuzheng Si, Wentao Ma, Yuchuan Wu, Yinpei Dai, Haoyu Gao, Ting-En Lin, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2305.13040.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [NeurIPS 2023] </b>
                </li><br>

                <li>
                    <b> Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch. </b>
                    <br>
                    Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2311.03099.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [submitting] </b>
                </li><br>

                <li>
                    <b> Wider and deeper llm networks are fairer llm evaluators. </b>
                    <br>
                    Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2308.01862.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [submitting] </b>
                </li><br>

                <li>
                    <b> Improving Situated Conversational Agents with Step-by-Step Multi-modal Logic Reasoning. </b>
                    <br>
                    Yuxing Long, Huibin Zhang, Binyuan Hui, Zhenglu Yang, Caixia Yuan, Xiaojie Wang, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://aclanthology.org/2023.dstc-1.3.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [SigDial 2023 DSTC11 Workshop Best Paper] </b>
                </li><br>
                
                <li>
                    <b> Iterative Forward Tuning Boosts In-context Learning in Language Models. </b>
                    <br>
                    Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2305.13016.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [submitting] </b>
                </li><br>
                
                <li>
                    <b> Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning. </b>
                    <br>
                    Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2301.13808.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [SIGIR 2023] </b>
                </li><br>

                <li>
                    <b> A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions. </b>
                    <br>
                    Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2208.13629.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [TKDE 2022] </b>
                </li><br>
                
                <li>
                    <b> UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition. </b>
                    <br>
                    Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, Yongbin Li
                    <br>
                    [<a href="https://arxiv.org/pdf/2211.11256.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> Towards Generalizable and Robust Text-to-SQL Parsing. </b>
                    <br>
                    Chang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Binhua Li, Fei Huang, Luo Si, Yongbin Li
                    <br>
                    [<a href="https://arxiv.org/pdf/2210.12674.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation. </b>
                    <br>
                    Yinpei Dai, Wanwei He, Bowen Li, Yuchuan Wu, Zheng Cao, Zhongqi An, Jian Sun, Yongbin Li
                    <br>
                    [<a href="https://arxiv.org/pdf/2211.11617.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings. </b>
                    <br>
                    Che Liu, Rui Wang, Junfeng Jiang, Yongbin Li, Fei Huang
                    <br>
                    [<a href="https://arxiv.org/pdf/2210.15332.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> Prompt Conditioned VAE: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue. </b>
                    <br>
                    Yingxiu Zhao, Yinhe Zheng, Zhiliang Tian, Chang Gao, Bowen Yu, Haiyang Yu, Yongbin Li, Jian Sun, Nevin L Zhang
                    <br>
                    [<a href="https://arxiv.org/pdf/2210.07783.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots. </b>
                    <br>
                    Haomin Fu, Yeqin Zhang, Haiyang Yu, Jian Sun, Fei Huang, Luo Si, Yongbin Li, Cam-Tu Nguyen
                    <br>
                    [<a href="https://arxiv.org/pdf/2210.11060.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing. </b>
                    <br>
                    Zefeng Cai, Xiangyu Li, Binyuan Hui, Min Yang, Bowen Li, Binhua Li, Zheng Cao, Weijie Li, Fei Huang, Luo Si, Yongbin Li
                    <br>
                    [<a href="https://arxiv.org/pdf/2210.11888.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2022] </b>
                </li><br>

                <li>
                    <b> Graph-to-Text Generation with Dynamic Structure Pruning. </b>
                    <br>
                    Liang Li, Ruiying Geng, Bowen Li, Can Ma, Yinliang Yue, Binhua Li, Yongbin Li
                    <br>
                    [<a href="https://arxiv.org/pdf/2209.07258.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [COLING 2022] </b>
                </li><br>

                <li>
                    <b>Proton: Probing Schema Linking Information from Pre-trained Language Models for Text-to-SQL Parsing. </b>
                    <br>
                    Lihan Wang*, Bowen Qin*, Binyuan Hui*, Bowen Li, Ming Yang, Bailin Wang, Binhua Li, Fei Huang, Luo Si, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/abs/2206.14017">Paper</a>]
                    [<a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/proton">Code</a>]
                    <b style="color: 	#FF4500;"> [KDD 2022] </b>
                </li><br>

                <li>
                    <b>Duplex Conversation: Enable Human-like Interaction in Spoken Dialogue System. </b>
                    <br>
                    Ting-En Lin, Yuchuan Wu, Fei Huang, Luo Si, Jian Sun and Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2205.15060.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [KDD 2022] </b>
                </li><br>

                <li>
                    <b> S¬≤SQL: Injecting Syntax to Question-schema Interaction Graph Encoder for Text-to-SQL Parsers. </b>
                    <br>
                    Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Bowen Li, Jian Sun, Yongbin Li<br>
                    
                    [<a href="https://arxiv.org/pdf/2203.06958.pdf">Paper</a>]
                    [<a href="">Code</a>]
                    <b style="color: 	#FF4500;"> [ACL 2022 Findings] </b>
                </li><br>

                 <li>
                    <b>A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots. </b>
                    <br>
                    Sai Zhang, Yuwei Hu, Yuchuan Wu, Jiaman Wu, Yongbin Li, Jian SUN, Caixia Yuan and  Xiaojie Wang
                    <br>

                    [<a href="https://arxiv.org/pdf/2203.10759.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [ACL 2022 Findings] </b>
                </li><br>

                 <li>
                    <b>A Survey on Neural Open Information Extraction: Current Status and Future Directions. </b>
                    <br>
                    Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, Haiyang Yu, Jian Sun, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2205.11725.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [IJCAI 2022] </b>
                </li><br>

                <li>
                    <b>Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration. </b>
                    <br>
                    Zhenyu Zhang, Bowen Yu, Haiyang Yu, Tingwen Liu, Cheng Fu, Jingyang Li, Chengguang Tang, Jian Sun, Yongbin Li
                    <br>

                    [<a href="https://arxiv.org/pdf/2207.06717.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [ACM MM 2022] </b>
                </li><br>

                <li>
                    <b>Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking. </b>
                    <br>
                    Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si and Xiaodan Zhu
                    <br>

                    [<a href="https://arxiv.org/pdf/2106.00291.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [ACL 2021] </b>
                </li><br>

                <li>
                    <b> R¬≤SQL: Dynamic Hybrid Relation Network for Cross-Domain Context-Dependent Semantic Parsing. </b>
                    <br>
                    Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei Zhu, Xiaodan Zhu
                    <br>
                    
                    [<a href="https://arxiv.org/pdf/2101.01686.pdf">Paper</a>]
                    [<a href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/r2sql">Code</a>]
                    <b style="color: 	#FF4500;"> [AAAI 2021] </b>
                </li><br>

                <li>
                    <b> Improving text-to-sql with schema dependency learning. </b>
                    <br>
                    Binyuan Hui, Xiang Shi, Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu
                    <br>

                    [<a href="https://arxiv.org/pdf/2103.04399.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [arXiv] </b>
                </li><br>



                <li>
                    <b>Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment. </b>
                    <br>
                    Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin Li, Jian Sun, Xiaodan Zhu
                    <br>

                    [<a href="https://aclanthology.org/2020.acl-main.57.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [ACL 2020] </b>
                </li><br>

                <li>
                    <b>Dynamic Memory Induction Networks for Few-Shot Text Classification. </b>
                    <br>
                    Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu
                    <br>

                    [<a href="https://arxiv.org/pdf/2005.05727.pdf">Paper</a>]
                    <!-- [<a href="https://github.com/facebookresearch/ELECTRA-Fewshot-Learning">Code: Commong soon...</a>] -->
                    <b style="color: 	#FF4500;"> [ACL 2020] </b>
                 </li><br>

                <li>
                    <b>Induction Networks for Few-Shot Text Classification. </b>
                    <br>
                    Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun
                    <br>

                    [<a href="https://arxiv.org/pdf/1902.10482.pdf">Paper</a>]
                    <b style="color: 	#FF4500;"> [EMNLP 2019] </b>
                </li><br>
                
            </ul>
        </div>

        <div id="footer">
            <p style="text-align:right;">Last modified date: February, 2024
        </div>
    </div>
</body>
<div id="edge-translate-notifier-container" class="edge-translate-notifier-center"></div>
</html>
